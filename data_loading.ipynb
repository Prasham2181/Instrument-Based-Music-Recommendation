{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "U-9Kz9qjGYcd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import librosa\n",
    "import scipy.ndimage as ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RS1sQekbGYcf"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KyclL4XoGYcg"
   },
   "outputs": [],
   "source": [
    "# Paths and dimensions\n",
    "input_dir = os.path.join('Final_Dataset', 'Input')\n",
    "output_dir = os.path.join('Final_Dataset','Output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZMJzNUChGYcg"
   },
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class UNetDataset(Dataset):\n",
    "    def __init__(self, input_dir, output_dir, transform=None, target_transform=None, image_size=(512, 512)):\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.image_size = image_size\n",
    "\n",
    "        # List all input files\n",
    "        self.input_files = sorted(os.listdir(input_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load input image\n",
    "        input_file = self.input_files[idx]\n",
    "        input_path = os.path.join(self.input_dir, input_file)\n",
    "        input_image = Image.open(input_path).convert('L')  # Convert to grayscale\n",
    "\n",
    "        # Load corresponding output folder\n",
    "        track_name = input_file.split('_mix')[0]\n",
    "        output_folder = os.path.join(self.output_dir, track_name)\n",
    "        output_files = sorted(os.listdir(output_folder))\n",
    "\n",
    "        # Load and stack output images\n",
    "        output_images = []\n",
    "        for output_file in output_files:\n",
    "            output_path = os.path.join(output_folder, output_file)\n",
    "            output_image = Image.open(output_path).convert('L')  # Convert to grayscale\n",
    "            output_images.append(output_image)\n",
    "\n",
    "        # Resize input and output images\n",
    "        if self.image_size:\n",
    "            input_image = input_image.resize(self.image_size)\n",
    "            output_images = [img.resize(self.image_size) for img in output_images]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "        else:\n",
    "            input_image = transforms.ToTensor()(input_image)  # Default transform to tensor\n",
    "\n",
    "        if self.target_transform:\n",
    "            output_images = [self.target_transform(img) for img in output_images]\n",
    "        else:\n",
    "            output_images = [transforms.ToTensor()(img) for img in output_images]\n",
    "\n",
    "        # Stack output images along the channel axis\n",
    "        output_tensor = torch.cat(output_images, dim=0)\n",
    "\n",
    "        return input_image, output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SJBy4HQrGYch"
   },
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EvDFSpBtGYch"
   },
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = UNetDataset(input_dir, output_dir, transform=transform)\n",
    "\n",
    "# DataLoader for batching and shuffling\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xp3zkKpnGYci",
    "outputId": "b5bcd95d-fda8-4aab-9f4b-e4febf7f262d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([4, 1, 512, 512]), Output shape: torch.Size([4, 5, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "for inputs, outputs in dataloader:\n",
    "    print(f\"Input shape: {inputs.shape}, Output shape: {outputs.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "17OwRA2YGYck"
   },
   "outputs": [],
   "source": [
    "# Unet model\n",
    "\n",
    "class UNET(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder part of unet\n",
    "        self.encoder1 = self.conv_block(in_channels, 32)\n",
    "        self.encoder2 = self.conv_block(32, 64)\n",
    "        self.encoder3 = self.conv_block(64, 128)\n",
    "        self.encoder4 = self.conv_block(128, 256)\n",
    "        self.encoder5 = self.conv_block(256, 512)\n",
    "\n",
    "        # bottleneck layer\n",
    "        self.bottleneck = self.conv_block(512, 1024)\n",
    "\n",
    "        # Decoder part of unet\n",
    "        self.upsampling5 = self.upsampling_block(1024, 512)\n",
    "        self.decoder5 = self.conv_block(1024, 512)\n",
    "        self.upsampling4 = self.upsampling_block(512, 256)\n",
    "        self.decoder4 = self.conv_block(512, 256)\n",
    "        self.upsampling3 = self.upsampling_block(256, 128)\n",
    "        self.decoder3 = self.conv_block(256, 128)\n",
    "        self.upsampling2 = self.upsampling_block(128, 64)\n",
    "        self.decoder2 = self.conv_block(128, 64)\n",
    "        self.upsampling1 = self.upsampling_block(64, 32)\n",
    "        self.decoder1 = self.conv_block(64, 32)\n",
    "\n",
    "\n",
    "        # changing to desired number of channels\n",
    "        self.output = nn.Conv2d(32, out_channels, kernel_size=1)\n",
    "\n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        conv =  nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        return conv\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        # Encoder part of unet\n",
    "        encoder1 = self.encoder1(input)\n",
    "        encoder2 = self.encoder2(nn.MaxPool2d(2)(encoder1))\n",
    "        encoder3 = self.encoder3(nn.MaxPool2d(2)(encoder2))\n",
    "        encoder4 = self.encoder4(nn.MaxPool2d(2)(encoder3))\n",
    "        encoder5 = self.encoder5(nn.MaxPool2d(2)(encoder4))\n",
    "\n",
    "        # bottleneck layer\n",
    "        bottleneck = self.bottleneck(nn.MaxPool2d(2)(encoder5))\n",
    "\n",
    "        # decoder part of unet\n",
    "        decoder5 = self.upsampling5(bottleneck)\n",
    "        decoder5 = torch.cat((decoder5, encoder5), dim=1)\n",
    "        decoder5 = self.decoder5(decoder5)\n",
    "\n",
    "        decoder4 = self.upsampling4(decoder5)\n",
    "        decoder4 = torch.cat((decoder4, encoder4), dim=1)\n",
    "        decoder4 = self.decoder4(decoder4)\n",
    "\n",
    "        decoder3 = self.upsampling3(decoder4)\n",
    "        decoder3 = torch.cat((decoder3, encoder3), dim=1)\n",
    "        decoder3 = self.decoder3(decoder3)\n",
    "\n",
    "        decoder2 = self.upsampling2(decoder3)\n",
    "        decoder2 = torch.cat((decoder2, encoder2), dim=1)\n",
    "        decoder2 = self.decoder2(decoder2)\n",
    "\n",
    "        decoder1 = self.upsampling1(decoder2)\n",
    "        decoder1 = torch.cat((decoder1, encoder1), dim=1)\n",
    "        decoder1 = self.decoder1(decoder1)\n",
    "\n",
    "        output = self.output(decoder1)\n",
    "        return output\n",
    "\n",
    "    def upsampling_block(self, in_channels, out_channels):\n",
    "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "duQTXu-TGYcl",
    "outputId": "295a5960-49ff-4e4a-e6a5-470bd179e539"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNET(\n",
      "  (encoder1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (encoder2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (encoder3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (encoder4): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (encoder5): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (bottleneck): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (upsampling5): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder5): Sequential(\n",
      "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (upsampling4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder4): Sequential(\n",
      "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (upsampling3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder3): Sequential(\n",
      "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (upsampling2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder2): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (upsampling1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (decoder1): Sequential(\n",
      "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (output): Conv2d(32, 5, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "in_channels, out_channels = 1, 5\n",
    "model = UNET(in_channels, out_channels).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "QYdiZpfMGYcm"
   },
   "outputs": [],
   "source": [
    "class EnergyBasedLossFunction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, predictions, targets, epsilon=1e-6):\n",
    "        \"\"\"\n",
    "        predictions: Tensor of shape (B, N, T), predicted signals\n",
    "        targets: Tensor of shape (B, N, T), ground truth signals\n",
    "        epsilon: Small constant to avoid division by zero\n",
    "        \"\"\"\n",
    "        # Compute MSE loss for each source in each sample\n",
    "        mse_loss = torch.mean((predictions - targets) ** 2, dim=-1) # Shape: (B, N)\n",
    "\n",
    "        # Compute energy for each source in each sample\n",
    "        energies = torch.sum(targets ** 2, dim=-1) # Shape: (B, N)\n",
    "\n",
    "        # Compute weights for each source in each sample\n",
    "        weights = 1.0 / (energies + epsilon) # Shape: (B, N)\n",
    "\n",
    "        # Compute weighted loss for each source in each sample\n",
    "        weighted_losses = weights * mse_loss # Shape: (B, N)\n",
    "\n",
    "        # Average over all sources and batch samples\n",
    "        total_loss = torch.mean(weighted_losses) # Scalar\n",
    "\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "K7GCurSaGYcm"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n",
    "loss_fn = EnergyBasedLossFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HWj72oogGYcn"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        loss, current = loss.item(), batch * len(X)\n",
    "        print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "r0Id0FtDGYcn"
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "def test(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, correct_preds = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct_preds += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "        test_loss /= num_batches\n",
    "        correct_preds /= size\n",
    "    print(\n",
    "        f\"Test Error: \\n Accuracy: {correct_preds*100:>7f}%, Avg loss: {test_loss:>8f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ElDSjb9GYcn",
    "outputId": "4a0dabee-e254-489c-e2e7-ea54654bb1d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------\n",
      "loss: 0.002027  [    0/   18]\n",
      "loss: 0.001996  [    4/   18]\n",
      "loss: 0.002024  [    8/   18]\n",
      "loss: 0.001966  [   12/   18]\n",
      "loss: 0.001978  [    8/   18]\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------\")\n",
    "    train(dataloader, model, loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JRSxHjRHKcIz"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('Models'):\n",
    "    os.makedirs('Models')\n",
    "    \n",
    "torch.save(model.state_dict(), os.path.join('Models', 'model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_source_masks(model, spectrogram_image_path):\n",
    "    # Loading the image\n",
    "    img = Image.open(spectrogram_image_path).convert('L')\n",
    "    \n",
    "    # Creating a transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),  \n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    ])\n",
    "    \n",
    "    # Transforming the input image\n",
    "    input_tensor = transform(img)\n",
    "    \n",
    "    # Adding batch dimension at the first position\n",
    "    input_tensor = input_tensor.unsqueeze(0).to(device)\n",
    "    \n",
    "    # Model in evaluation model\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        softmasks = model(input_tensor).detach().numpy()\n",
    "    \n",
    "    return softmasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the length of every audio equal to 180 seconds\n",
    "def make_lengths_same(audio_file, sample_rate, target_duration=180):\n",
    "    try:\n",
    "        # Finding the length of input audio\n",
    "        audio_length = len(audio_file)\n",
    "\n",
    "        # Finding the target length in number of samples\n",
    "        target_length = int(sample_rate * target_duration)\n",
    "\n",
    "        if audio_length < target_length:  # If audio duration is less than 180 seconds\n",
    "            padding = target_length - audio_length # Finding how much padding is required\n",
    "            padding_left = 0  # Padding with zero\n",
    "            padding_right = padding # Padding from the right side\n",
    "            audio_file = np.pad(audio_file, (padding_left, padding_right), mode='constant', constant_values=0) # Padding\n",
    "\n",
    "        elif audio_length > target_length: # If audio duration is greater than 180 seconds\n",
    "            audio_file = audio_file[:target_length] # Cutting down the excess audio\n",
    "\n",
    "        return audio_file\n",
    "    except Exception as e:\n",
    "        print(f\"Error encounterd in the function 'make_lengths_same'.\")\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_spectrogram_db(spectrogram, target_shape=(512, 512)):\n",
    "    return ndimage.zoom(spectrogram, (target_shape[0] / spectrogram.shape[0], target_shape[1] / spectrogram.shape[1]), order=3)\n",
    "\n",
    "def resample_spectrogram_phase(phase, target_shape=(512, 512)):\n",
    "    return ndimage.zoom(phase, (target_shape[0] / phase.shape[0], target_shape[1] / phase.shape[1]), order=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stft(wavform, n_fft=1022, hop_length=512, window_length=1022):\n",
    "    \n",
    "    stft_results = torch.stft(wavform, n_fft=1022, hop_length=hop_length, win_length=window_length, window=torch.hann_window(window_length), return_complex=True)\n",
    "    \n",
    "    # Computing magnitude and phase\n",
    "    magnitude = stft_results.abs()\n",
    "    phase = torch.angle(stft_results)\n",
    "\n",
    "    # Convert magnitude to decibels (log-compressed)\n",
    "    magnitude_db = 20 * torch.log10(magnitude + 1e-6)\n",
    "\n",
    "    # Normalize the magnitude spectrogram to range [0, 255] for grayscale\n",
    "    magnitude_db_normalized = (magnitude_db - magnitude_db.min()) / (magnitude_db.max() - magnitude_db.min()) * 255\n",
    "    magnitude_db_normalized = magnitude_db_normalized.squeeze().cpu().numpy().astype(np.uint8)\n",
    "    \n",
    "    magnitude_db_normalized = resample_spectrogram_db(magnitude_db_normalized, target_shape=(512, 512))\n",
    "    resampled_phase = resample_spectrogram_phase(phase)\n",
    "    \n",
    "    return magnitude_db_normalized, resampled_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def istft(magnitude, phase, n_fft=1022, hop_length=512, window_length=1022):\n",
    "    spec = magnitude * torch.exp(1j * torch.tensor(phase))\n",
    "    return torch.istft(spec, n_fft=1022, hop_length=hop_length, win_length=window_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def separate_sources(mixed_audio_waveform, softmask, n_fft=1022, hop_length=512, window_length=1024):\n",
    "    # Finding the magnitude and the phase of the mixed audio waveform\n",
    "    mixed_magnitude, mixed_phase = stft(mixed_audio_waveform)\n",
    "    \n",
    "    # Multiplying the mixed audio waveform magnitude with the source mask\n",
    "    masked_magnitude = torch.tensor(softmask) * torch.tensor(mixed_magnitude).unsqueeze(0)\n",
    "    \n",
    "    separated_sources = list()\n",
    "    \n",
    "    for index in range(softmask.shape[1]):  # Repeating this operation along the channel dimension\n",
    "        source_magnitude = masked_magnitude[:, index, :, :]\n",
    "        source_phase = mixed_phase \n",
    "        separate_source = istft(source_magnitude, source_phase)\n",
    "        separated_sources.append(separate_source)\n",
    "    \n",
    "    # Save each separated source to a .wav file\n",
    "    for i in range(len(separated_sources)):\n",
    "        waveform = separated_sources[i]\n",
    "        print(waveform.shape)\n",
    "        waveform = waveform.cpu().numpy()  # Convert to numpy array\n",
    "        \n",
    "        # Normalize waveform to the range [-1, 1] for 16-bit PCM audio\n",
    "        waveform = waveform.reshape(-1)  # Flatten to 1D if it's 1D already (samples,)\n",
    "        waveform = np.clip(waveform, -1.0, 1.0)  # Normalize to [-1, 1]\n",
    "        waveform = waveform.astype(np.float32)  # Convert to float32\n",
    "        \n",
    "        # Define sample rate (assuming it's 10880)\n",
    "        sr = 10880\n",
    "        \n",
    "        # Ensure the 'Outputs' directory exists\n",
    "        if not os.path.exists('Outputs'):\n",
    "            os.makedirs('Outputs')\n",
    "        \n",
    "        # Save the waveform to a .wav file\n",
    "        output_file = os.path.join('Outputs', f\"waveform_{i}.wav\")  # Ensure .wav extension\n",
    "        sf.write(output_file, waveform, sr)\n",
    "        print(f\"Saved waveform {i+1} to {output_file}\")\n",
    "        \n",
    "    return separated_sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 261632])\n",
      "Saved waveform 1 to Outputs\\waveform_0.wav\n",
      "torch.Size([1, 261632])\n",
      "Saved waveform 2 to Outputs\\waveform_1.wav\n",
      "torch.Size([1, 261632])\n",
      "Saved waveform 3 to Outputs\\waveform_2.wav\n",
      "torch.Size([1, 261632])\n",
      "Saved waveform 4 to Outputs\\waveform_3.wav\n",
      "torch.Size([1, 261632])\n",
      "Saved waveform 5 to Outputs\\waveform_4.wav\n"
     ]
    }
   ],
   "source": [
    "softmasks = predict_source_masks(model, os.path.join('Final_Dataset', 'Input', 'Track00001_mix.png'))\n",
    "\n",
    "y, sr = librosa.load(os.path.join('RawData', 'Track00005', 'mix.wav'), mono=True, sr=10880)\n",
    "y = make_lengths_same(y, sr)\n",
    "separated_sources = separate_sources(torch.tensor(y, dtype=torch.float32), softmasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "wpi_msds_assignments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
