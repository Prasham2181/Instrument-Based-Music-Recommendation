{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U-9Kz9qjGYcd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file_path = 'Spectrogram_Dataset.zip'\n",
        "extract_dir = 'Spectrogram_Dataset'\n",
        "\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)"
      ],
      "metadata": {
        "id": "eH9AimksHF6D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RS1sQekbGYcf"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KyclL4XoGYcg"
      },
      "outputs": [],
      "source": [
        "# Paths and dimensions\n",
        "input_dir = os.path.join('Spectrogram_Dataset', 'Spectrogram_Dataset', 'Input')\n",
        "output_dir = os.path.join('Spectrogram_Dataset', 'Spectrogram_Dataset','Output')\n",
        "height, width = 500, 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZMJzNUChGYcg"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class UNetDataset(Dataset):\n",
        "    def __init__(self, input_dir, output_dir, transform=None, target_transform=None, image_size=(256, 256)):\n",
        "        self.input_dir = input_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.image_size = image_size\n",
        "\n",
        "        # List all input files\n",
        "        self.input_files = sorted(os.listdir(input_dir))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load input image\n",
        "        input_file = self.input_files[idx]\n",
        "        input_path = os.path.join(self.input_dir, input_file)\n",
        "        input_image = Image.open(input_path).convert('L')  # Convert to grayscale\n",
        "\n",
        "        # Load corresponding output folder\n",
        "        track_name = input_file.split('_mix')[0]\n",
        "        output_folder = os.path.join(self.output_dir, track_name)\n",
        "        output_files = sorted(os.listdir(output_folder))\n",
        "\n",
        "        # Load and stack output images\n",
        "        output_images = []\n",
        "        for output_file in output_files:\n",
        "            output_path = os.path.join(output_folder, output_file)\n",
        "            output_image = Image.open(output_path).convert('L')  # Convert to grayscale\n",
        "            output_images.append(output_image)\n",
        "\n",
        "        # Resize input and output images\n",
        "        if self.image_size:\n",
        "            input_image = input_image.resize(self.image_size)\n",
        "            output_images = [img.resize(self.image_size) for img in output_images]\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            input_image = self.transform(input_image)\n",
        "        else:\n",
        "            input_image = transforms.ToTensor()(input_image)  # Default transform to tensor\n",
        "\n",
        "        if self.target_transform:\n",
        "            output_images = [self.target_transform(img) for img in output_images]\n",
        "        else:\n",
        "            output_images = [transforms.ToTensor()(img) for img in output_images]\n",
        "\n",
        "        # Stack output images along the channel axis\n",
        "        output_tensor = torch.cat(output_images, dim=0)\n",
        "\n",
        "        return input_image, output_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SJBy4HQrGYch"
      },
      "outputs": [],
      "source": [
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EvDFSpBtGYch"
      },
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "dataset = UNetDataset(input_dir, output_dir, transform=transform)\n",
        "\n",
        "# DataLoader for batching and shuffling\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Xp3zkKpnGYci",
        "outputId": "b5bcd95d-fda8-4aab-9f4b-e4febf7f262d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([3, 1, 256, 256]), Output shape: torch.Size([3, 5, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "# Check\n",
        "for inputs, outputs in dataloader:\n",
        "    print(f\"Input shape: {inputs.shape}, Output shape: {outputs.shape}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "17OwRA2YGYck"
      },
      "outputs": [],
      "source": [
        "# Unet model\n",
        "\n",
        "class UNET(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder part of unet\n",
        "        self.encoder1 = self.conv_block(in_channels, 32)\n",
        "        self.encoder2 = self.conv_block(32, 64)\n",
        "        self.encoder3 = self.conv_block(64, 128)\n",
        "        self.encoder4 = self.conv_block(128, 256)\n",
        "        self.encoder5 = self.conv_block(256, 512)\n",
        "\n",
        "        # bottleneck layer\n",
        "        self.bottleneck = self.conv_block(512, 1024)\n",
        "\n",
        "        # Decoder part of unet\n",
        "        self.upsampling5 = self.upsampling_block(1024, 512)\n",
        "        self.decoder5 = self.conv_block(1024, 512)\n",
        "        self.upsampling4 = self.upsampling_block(512, 256)\n",
        "        self.decoder4 = self.conv_block(512, 256)\n",
        "        self.upsampling3 = self.upsampling_block(256, 128)\n",
        "        self.decoder3 = self.conv_block(256, 128)\n",
        "        self.upsampling2 = self.upsampling_block(128, 64)\n",
        "        self.decoder2 = self.conv_block(128, 64)\n",
        "        self.upsampling1 = self.upsampling_block(64, 32)\n",
        "        self.decoder1 = self.conv_block(64, 32)\n",
        "\n",
        "\n",
        "        # changing to desired number of channels\n",
        "        self.output = nn.Conv2d(32, out_channels, kernel_size=1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        conv =  nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding='same'),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding='same'),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        return conv\n",
        "\n",
        "    def forward(self, input):\n",
        "\n",
        "        # Encoder part of unet\n",
        "        encoder1 = self.encoder1(input)\n",
        "        encoder2 = self.encoder2(nn.MaxPool2d(2)(encoder1))\n",
        "        encoder3 = self.encoder3(nn.MaxPool2d(2)(encoder2))\n",
        "        encoder4 = self.encoder4(nn.MaxPool2d(2)(encoder3))\n",
        "        encoder5 = self.encoder5(nn.MaxPool2d(2)(encoder4))\n",
        "\n",
        "        # bottleneck layer\n",
        "        bottleneck = self.bottleneck(nn.MaxPool2d(2)(encoder5))\n",
        "\n",
        "        # decoder part of unet\n",
        "        decoder5 = self.upsampling5(bottleneck)\n",
        "        decoder5 = torch.cat((decoder5, encoder5), dim=1)\n",
        "        decoder5 = self.decoder5(decoder5)\n",
        "\n",
        "        decoder4 = self.upsampling4(decoder5)\n",
        "        decoder4 = torch.cat((decoder4, encoder4), dim=1)\n",
        "        decoder4 = self.decoder4(decoder4)\n",
        "\n",
        "        decoder3 = self.upsampling3(decoder4)\n",
        "        decoder3 = torch.cat((decoder3, encoder3), dim=1)\n",
        "        decoder3 = self.decoder3(decoder3)\n",
        "\n",
        "        decoder2 = self.upsampling2(decoder3)\n",
        "        decoder2 = torch.cat((decoder2, encoder2), dim=1)\n",
        "        decoder2 = self.decoder2(decoder2)\n",
        "\n",
        "        decoder1 = self.upsampling1(decoder2)\n",
        "        decoder1 = torch.cat((decoder1, encoder1), dim=1)\n",
        "        decoder1 = self.decoder1(decoder1)\n",
        "\n",
        "        output = self.output(decoder1)\n",
        "        return output\n",
        "\n",
        "    def upsampling_block(self, in_channels, out_channels):\n",
        "        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "duQTXu-TGYcl",
        "outputId": "295a5960-49ff-4e4a-e6a5-470bd179e539",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNET(\n",
            "  (encoder1): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (encoder2): Sequential(\n",
            "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (encoder3): Sequential(\n",
            "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (encoder4): Sequential(\n",
            "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (encoder5): Sequential(\n",
            "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (bottleneck): Sequential(\n",
            "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (upsampling5): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "  (decoder5): Sequential(\n",
            "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (upsampling4): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "  (decoder4): Sequential(\n",
            "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (upsampling3): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "  (decoder3): Sequential(\n",
            "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (upsampling2): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "  (decoder2): Sequential(\n",
            "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (upsampling1): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "  (decoder1): Sequential(\n",
            "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (output): Conv2d(32, 5, kernel_size=(1, 1), stride=(1, 1))\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "in_channels, out_channels = 1, 5\n",
        "model = UNET(in_channels, out_channels).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "QYdiZpfMGYcm"
      },
      "outputs": [],
      "source": [
        "class EnergyBasedLossFunction(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, predictions, targets, epsilon=1e-6):\n",
        "        \"\"\"\n",
        "        predictions: Tensor of shape (B, N, T), predicted signals\n",
        "        targets: Tensor of shape (B, N, T), ground truth signals\n",
        "        epsilon: Small constant to avoid division by zero\n",
        "        \"\"\"\n",
        "        # Compute MSE loss for each source in each sample\n",
        "        mse_loss = torch.mean((predictions - targets) ** 2, dim=-1) # Shape: (B, N)\n",
        "\n",
        "        # Compute energy for each source in each sample\n",
        "        energies = torch.sum(targets ** 2, dim=-1) # Shape: (B, N)\n",
        "\n",
        "        # Compute weights for each source in each sample\n",
        "        weights = 1.0 / (energies + epsilon) # Shape: (B, N)\n",
        "\n",
        "        # Compute weighted loss for each source in each sample\n",
        "        weighted_losses = weights * mse_loss # Shape: (B, N)\n",
        "\n",
        "        # Average over all sources and batch samples\n",
        "        total_loss = torch.mean(weighted_losses) # Scalar\n",
        "\n",
        "        return total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "K7GCurSaGYcm"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n",
        "loss_fn = EnergyBasedLossFunction()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "HWj72oogGYcn"
      },
      "outputs": [],
      "source": [
        "# training\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "r0Id0FtDGYcn"
      },
      "outputs": [],
      "source": [
        "# testing\n",
        "def test(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    model.eval()\n",
        "    test_loss, correct_preds = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct_preds += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "        test_loss /= num_batches\n",
        "        correct_preds /= size\n",
        "    print(\n",
        "        f\"Test Error: \\n Accuracy: {correct_preds*100:>7f}%, Avg loss: {test_loss:>8f}\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0ElDSjb9GYcn",
        "outputId": "4a0dabee-e254-489c-e2e7-ea54654bb1d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------\n",
            "loss: 0.004181  [    0/    3]\n",
            "Epoch 2\n",
            "-------------------------\n",
            "loss: 0.004130  [    0/    3]\n",
            "Epoch 3\n",
            "-------------------------\n",
            "loss: 0.004087  [    0/    3]\n",
            "Epoch 4\n",
            "-------------------------\n",
            "loss: 0.004050  [    0/    3]\n",
            "Epoch 5\n",
            "-------------------------\n",
            "loss: 0.004021  [    0/    3]\n",
            "Epoch 6\n",
            "-------------------------\n",
            "loss: 0.003996  [    0/    3]\n",
            "Epoch 7\n",
            "-------------------------\n",
            "loss: 0.003973  [    0/    3]\n",
            "Epoch 8\n",
            "-------------------------\n",
            "loss: 0.003952  [    0/    3]\n",
            "Epoch 9\n",
            "-------------------------\n",
            "loss: 0.003933  [    0/    3]\n",
            "Epoch 10\n",
            "-------------------------\n",
            "loss: 0.003915  [    0/    3]\n",
            "Epoch 11\n",
            "-------------------------\n",
            "loss: 0.003899  [    0/    3]\n",
            "Epoch 12\n",
            "-------------------------\n",
            "loss: 0.003883  [    0/    3]\n",
            "Epoch 13\n",
            "-------------------------\n",
            "loss: 0.003868  [    0/    3]\n",
            "Epoch 14\n",
            "-------------------------\n",
            "loss: 0.003853  [    0/    3]\n",
            "Epoch 15\n",
            "-------------------------\n",
            "loss: 0.003838  [    0/    3]\n"
          ]
        }
      ],
      "source": [
        "epochs = 15\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch + 1}\\n-------------------------\")\n",
        "    train(dataloader, model, loss_fn, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ],
      "metadata": {
        "id": "JRSxHjRHKcIz"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the image\n",
        "image_path = 'path_to_your_image.jpg'  # Path to the image you want to classify\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Define the image transformation (e.g., resize, to tensor, normalization)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize image to 224x224 for models like ResNet\n",
        "    transforms.ToTensor(),  # Convert image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Pre-trained ImageNet normalization\n",
        "])\n",
        "\n",
        "# Apply transformations to the image\n",
        "input_tensor = transform(image)\n",
        "\n",
        "# Add batch dimension (as models expect a batch, even for one image)\n",
        "input_tensor = input_tensor.unsqueeze(0)\n"
      ],
      "metadata": {
        "id": "wjmQTx0jKdwo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}